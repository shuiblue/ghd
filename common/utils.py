
# from __future__ import unicode_literals

import networkx as nx
import pandas as pd

from collections import defaultdict
import datetime
import logging

from common import decorators as d
from common import mapreduce
from common import versions
import scraper

# ecosystems
import npm
import pypi

ECOSYSTEMS = {
    'npm': npm,
    'pypi': pypi
}

logger = logging.getLogger("ghd")
fs_cache = d.fs_cache('common')

# default start dates for ecosystem datasets. It is used for sanity checks
START_DATES = {
    'npm': '2010',
    'pypi': '2005'
}

""" This lookup is used by parse_license()
Since many license strings contain several (often conflicting) licenses,
the least restrictive license takes precedence.

https://en.wikipedia.org/wiki/Comparison_of_free_and_open-source_software_licenses
"""

LICENSE_TYPES = (
    (  # permissive
        ('apache', 'Apache'),
        ('isc', 'ISC'),
        ('mit', 'MIT'),
        ('bsd', 'BSD'),
        ('wtf', 'WTFPL'),
        ('unlicense', 'PD'),
    ),
    (  # somewhat restrictive
        ('mozilla', 'MPL'),
        # 'mpl' will also match words like 'simple' and 'example'
    ),
    (  # somewhat permissive
        ('lesser', 'LGPL'),
        ('lgpl', 'LGPL'),
    ),
    (  # strong copyleft
        ('general public', 'GPL'),
        ('gpl', 'GPL'),
        ('affero', 'GPL'),
        ('CC-BY-SA', 'CC-BY-SA'),
    ),
    (  # permissive again
        ('public', 'PD'),
        ('CC-BY', 'CC'),
        ('creative', 'CC'),
    ),
)


def get_ecosystem(ecosystem):
    """ Return ecosystem module if supported, raise ValueError otherwise """
    if ecosystem not in ECOSYSTEMS:
        raise ValueError(
            "Ecosystem %s is not supported. Only (%s) are supported so far" % (
                ecosystem, ",".join(ECOSYSTEMS.keys())))
    return ECOSYSTEMS[ecosystem]


@fs_cache
def package_urls(ecosystem):
    # type: (str) -> pd.Series
    """ A shortcut to get list of packages having identified repo URL
    Though it looks trivial, it is a rather important method.
    >>> urls = package_urls("pypi")
    >>> isinstance(urls, pd.Series)
    True
    >>> 50000 < len(urls) < 200000
    True
    >>> "django" in urls
    True
    """
    es = get_ecosystem(ecosystem)
    urls = es.packages_info()["url"].dropna()

    def supported(url):
        try:
            scraper.get_provider(url)
        except NotImplementedError:
            return False
        return True

    urls = urls[urls.map(supported)]

    # this part normalizes URLs, e.g. by removing trailing .git from GitHub URLs
    def normalize(url):
        provider, project_url = scraper.get_provider(url)
        return provider.canonical_url(project_url)

    urls = urls.map(normalize)

    # this is necessary to get rid of false URLS, such as:
    # - meta-urls, e.g.
    #       http://github.com/npm/deprecate-holder.git
    #       http://github.com/npm/security-holder.git
    # - foundries, i.e. repositories hosting swarms of packages at once
    #       github.com/micropython/micropython-lib
    #       https://bitbucket.org/ronaldoussoren/pyobjc/src
    # - false records generated by code generators
    #       "This project was generated with angular-cli"
    #       "This project was bootstrapped with [Create React App]"
    #       github.com/swagger-api/swagger-codegen
    # NPM: 446K -> 389K
    # PyPI: 91728 -> 86892
    urls = urls[urls.map(urls.value_counts()) == 1]

    def exists(project_name, url):
        logger.info(project_name)
        provider, project_url = scraper.get_provider(url)
        return provider.project_exists(project_url)

    # - more than 16 threads make GitHub to choke even on public urls
    # - some malformed URLs will result in NaN (e.g. NPM abwa-gulp and
    #       barco-jobs), so need to fillna()
    se = mapreduce.map(exists, urls, num_workers=16).fillna(False)

    return urls[se]


def get_repo_usernames(urls):
    # type: (pd.Series) -> pd.DataFrame
    """
    This function is used by user_info to extract name of repository owner
    from its URL. It works so far, but violates abstraction and should be
    refactored at some point
    :param: pd.Series with urls, e.g. github.com/pandas-dev/pandas
    :return pd.Dataframe with columns
        index: url index, package name if package_urls() is used as inputs
        - provider_name: str, {github.com|bitbucket.org|gitlab.com}
        - login: str, provider-specific login

    >>> urls = s = pd.Series([
    ...     "github.com/pandas-dev/pandas",
    ...     "github.com/user2589/ghd",
    ...     "github.com/dkhsd/asdf"])
    >>> usernames = get_repo_usernames(urls)
    >>> isinstance(usernames, pd.DataFrame)
    True
    >>> len(urls) == len(usernames)
    True
    >>> all(col in usernames.columns for col in ('provider_name', 'login'))
    True
    >>> usernames.loc["pandas", "provider_name"]
    'github.com'
    >>> usernames.loc['pandas', 'login']
    'pandas-dev'
    """
    def gen():
        for _, url in urls.items():
            provider_name, project_url = scraper.parse_url(url)
            # Assuming urls come from package_urls,
            # we already know the provider is supported
            yield {
                'provider_name': provider_name,
                'login': project_url.split("/", 1)[0]
            }
    return pd.DataFrame(gen(), index=urls.index.rename("name"))


@fs_cache
def user_info(ecosystem):
    # type: (str) -> pd.DataFrame
    """ Return user profile fields
    Originally this method was created to differentiate org from user accounts

    :param ecosystem: {npm|pypi}
    :return: pd.DataFrame:
        Index: (provider, username)
        - provider_name: {github.com|bitbucket.org|gitlab.com}
        - login: username on the provider website
        - created_at: str, ISO timestamp
        - org: bool, whether it is an organization account (vs personal)
        - public_repos: int
        - followers: int
        - following: int
    >>> ui = user_info("pypi")
    >>> isinstance(ui, pd.DataFrame)
    True
    >>> 50000 < len(ui) < 200000  # 59192 as of Jan 18
    True
    >>> ui.loc["pandas", "org"]
    True
    >>> ui.loc["django", "org"]
    True
    >>> ui.loc["minicms", "org"]
    False
    """

    def get_user_info(_, row):
        # single column dataframe is used instead of series to simplify
        # result type conversion
        username = row["login"]
        logger.info("Processing %s", username)
        fields = ['created_at', 'login', 'type', 'public_repos',
                  'followers', 'following']
        provider_name, _ = scraper.parse_url(row["url"])
        provider, _ = scraper.get_provider(row["url"])
        try:
            data = provider.user_info(username)
        except scraper.RepoDoesNotExist:
            return {}
        res = {field: data.get(field) for field in fields}
        res["provider_name"] = provider_name
        return res

    # Since we're going to get many fields out of one, to simplify type
    # conversion it makes sense to convert to pd.DataFrame.
    # by the same reason, user_info() above gets row and not url value
    urls = package_urls(ecosystem)
    urls.index = urls  # will need it in get_user_info
    # it's going to be a pd.DataFrame(provider_name, login, url)
    usernames = get_repo_usernames(urls).reset_index()

    # ensure uniqueness of (provider, login) pairs to avoid extra requests
    # GitHub seems to ban IP (will get HTTP 403) if use 8 workers
    ui = mapreduce.map(
        get_user_info,
        usernames.groupby(["provider_name", "login"]).first().reset_index(),
        num_workers=6)

    # TODO: move to provider
    ui["org"] = ui["type"].map({"Organization": True, "User": False})
    ui = ui.drop(["type"], axis=1).set_index(
        ["provider_name", "login"], drop=True)
    # remove duplicated NaN rows for missing / deleted accounts
    ui = ui[~ui.index.duplicated()]

    def map2project(row):
        try:
            return ui.loc[row["provider_name"], row["login"]]
        except KeyError:
            return {}

    # finally, reindex by project instead of (provider, login)
    urls = package_urls(ecosystem)
    usernames = get_repo_usernames(urls)
    return usernames.apply(map2project, axis=1).dropna()


def parse_license(license):
    """ Standardize raw license string.
    - If two licenses specified, more permissive prevails.
    :param license: raw string
    :return str, {Apache|ISC|MIT|BSD|WTFPL|PD|MPL|LGPL|GPL|CC|CC-BY-SA}
            None if license is not recognized

    ~1 second for NPM, no need to cache
    - 3295 unique values in PyPI (lowercase for normalization)
    + gpl + general public - lgpl - lesser = 575 + 152 - 152 + 45 = 530
        includes affero
    + bsd: 358
    + mit: 320
    + lgpl + lesser = 152 + 45 = 197
    + apache: 166
    + creative: 44
    + domain: 34
    + mpl - simpl - mple = 29
    + zpl: 26
    + wtf: 22
    + zlib: 7
    - isc: just a few, but MANY in NPM
    - copyright: 763
        "copyright" is often (50/50) used with "mit"

    >>> parse_license("shared under LGPL")
    'LGPL'
    >>> parse_license("shared under LGPL and MIT")  # MIT is more permissive
    'MIT'
    >>> parse_license("creative commons")
    'CC'
    >>> parse_license("GNU Public License")  # check it is not public domain
    'GPL'
    >>> parse_license("lesser GNU Public License")
    'LGPL'
    >>> parse_license("MIT and Public domain")
    'PD'
    """
    if license and pd.notnull(license):
        license = license.lower()
        # the most permissive ones come first
        for license_types in LICENSE_TYPES:
            for token, license_type in license_types:
                if token in license:
                    return license_type
    return None


def count_values(df):
    # type: (pd.DataFrame) -> pd.DataFrame
    """ Count number of values in lists/sets
    It is initially introduced to count dependencies
    >>> c = count_values(pd.DataFrame({1:[set(), set(range(4)), [1,2,3,2,4]]}))
    >>> c.loc[0, 1]
    0
    >>> c.loc[1, 1]
    4
    >>> c.loc[2, 1]
    5
    """
    # takes around 20s for full pypi history

    def count(s):
        return len(s) if s and pd.notnull(s) else 0

    if isinstance(df, pd.DataFrame):
        return df.applymap(count)
    elif isinstance(df, pd.Series):
        return df.apply(count)


@d.memoize
def upstreams(ecosystem):
    # type: (str) -> pd.DataFrame
    """ Get a dataframe with upstream dependencies sliced per month
     ~66s for pypi, doesn't make sense to cache in filesystem

    :param ecosystem: str, {npm|pypi}
    :return pd.DataFrame, df.loc[package, month] = set([upstreams])

    >>> ups = upstreams("pypi")
    >>> isinstance(ups, pd.DataFrame)
    True
    >>> 50000 < len(ups) < 200000  # ~120K as of Jan 2018
    True
    >>> 150 < len(ups.columns) < 200  # number of month since Jan 2005
    True
    >>> ups.loc["django", "2017-12"] == {"pytz"}
    True
    """
    def gen():
        es = get_ecosystem(ecosystem)
        deps = es.dependencies().sort_values("date")
        # will drop 101 record out of 4M for npm
        deps = deps[deps["date"].notnull()]
        # otherwise, there is a package in NPM dated 1970 which increases
        # dataframe size manyfold
        deps = deps[deps["date"] > START_DATES[ecosystem]]
        deps['deps'] = deps['deps'].map(
            lambda x: set(x.split(",")) if x and pd.notnull(x) else set())
        # remove alpha releases, 835K-> 744K (PyPI)
        deps = deps[~(deps["version"].map(versions.is_alpha))]

        # for several releases per month, use the last value
        df = deps.groupby([deps.index, deps['date'].str[:7].rename('month')]
                          ).last().reset_index().sort_values(["name", "month"])

        last_release = ""
        last_package = ""
        for _, row in df.iterrows():
            if row["name"] != last_package:
                last_release = ""
                last_package = row["name"]
            # remove backports
            if versions.compare(row["version"], last_release) < 0:
                continue
            last_release = row["version"]
            yield row

    df = pd.DataFrame(gen(), columns=["name", "month", "deps"])

    # pypi was started around 2000, first meaningful numbers around 2005
    # npm was started Jan 2010, first meaningful release 2010-11
    # no need to cut off anything
    idx = [dt.strftime("%Y-%m")
           for dt in pd.date_range(df['month'].min(), 'now', freq="M")]

    dependencies = df.set_index(["name", "month"], drop=True)["deps"]
    # ffill can be dan with axis=1; Transpose here is to reindex
    return dependencies.unstack(level=0).reindex(idx).fillna(method='ffill').T


@d.memoize
def downstreams(ecosystem):
    # type: (str) -> pd.DataFrame
    """ Basically, reversed upstreams
    +25s to upstreams execution on PyPI dataset

    :param ecosystem: str, {pypi|npm}
    :return: pd.DataFrame, df.loc[project, month] = set([*projects])

    >>> ups = upstreams("pypi")
    >>> dss = downstreams("pypi")
    >>> isinstance(dss, pd.DataFrame)
    True
    >>> ups.shape == dss.shape
    True
    >>> all(ups.columns == dss.columns)
    True
    >>> all(ups.index == dss.index)
    True
    >>> 3500 < len(dss.loc["django", "2017-12"]) < 4000  # 3665
    True
    >>> dss.loc["django", "2007-12"] == {"pyswim"}
    True
    """
    uss = upstreams(ecosystem)

    def gen(row):
        s = defaultdict(set)
        for pkg, dss in row.items():
            if dss and pd.notnull(dss):
                # add package as downstream to each of upstreams
                for ds in dss:
                    s[ds].add(pkg)
        return pd.Series(s, name=row.name, index=row.index)

    return uss.apply(gen, axis=0)


def backporting(ecosystem, window=12):
    """
    In many cases "backporting" is caused by labeling errors so don't expect
    16s for PyPI

    How to test (face validity):
    pandas doesn't do backporting
    numpy did once (1.7.2, 2013-12-31)
        once they mislabeled a release (1.10.3 after 1.10.4, 2016-04-20)
    django does backport all the time (they always support at least couple
        most recent versions)

    :param ecosystem: str {npm|pypi}
    :param window: number of month to consider exercising backporting since
        observed
    :return: pd.Dataframe, df.loc[package, month] = <bool>

    >>> bp = backporting("pypi")
    >>> isinstance(bp, pd.DataFrame)
    True
    >>> 50000 < len(bp) < 200000
    True
    >>> 150 < len(bp.columns) < 200
    True
    >>> any(bp.loc['django', :'2017'])
    True
    >>> any(bp.loc['numpy', :'2017'])
    True
    >>> any(bp.loc['pandas', :'2017'])
    False
    >>> "2017-12" in bp.columns
    True
    """
    es = get_ecosystem(ecosystem)
    deps = es.dependencies().reset_index().sort_values(["name", "date"])
    projects = deps["name"].unique()
    deps = deps[~(deps["version"].map(versions.is_alpha))]
    deps["prev_version"] = deps["version"].shift(1)
    deps["prev_name"] = deps["name"].shift(1)
    deps = deps[deps["name"] == deps["prev_name"]]
    deps = deps[["name", "version", "prev_version", "date"]]
    deps["cmp"] = deps.apply(
            lambda row: versions.compare(row["version"], row["prev_version"]),
            axis=1)
    backported = deps.loc[deps["cmp"] < 0, ["name", "date"]]
    backported["date"] = backported["date"].str[:7]
    backported["backported"] = 1

    idx = [dt.strftime("%Y-%m")
           for dt in pd.date_range(backported['date'].min(), 'now', freq="M")]

    backported = backported.set_index(["name", "date"], drop=True)
    backported = backported.groupby(["name", "date"]).first()
    df = backported.unstack(level=0).reindex(idx).fillna(0)
    # level_0 is an artifact of multiindex
    df = df.T.reset_index().set_index("name", drop=True).drop("level_0", axis=1)
    df = df.rolling(window=window, min_periods=1, axis=1).mean()
    return df.reindex(projects, fill_value=0).astype(bool).astype(int)


def cumulative_dependencies(deps):
    """
   ~160 seconds for pypi upstreams, ?? for downstreams
   Tests:
         A      B
       /  \
      C    D
    /  \
   E    F
   >>> down = pd.DataFrame(
   ...     {1: [set(['c', 'd']), set(), set(['e', 'f']), set(), set(), set()]},
   ...     index=['a', 'b', 'c', 'd', 'e', 'f'])
   >>> len(cumulative_dependencies(down).loc['a', 1])
   5
   >>> len(cumulative_dependencies(down).loc['c', 1])
   2
   >>> len(cumulative_dependencies(down).loc['b', 1])
   0
   """
    def gen(dependencies):
        cumulative_upstreams = {}

        def traverse(pkg):
            if pkg not in cumulative_upstreams:
                cumulative_upstreams[pkg] = set()  # prevent infinite loop
                ds = dependencies[pkg]
                if ds and pd.notnull(ds):
                    cumulative_upstreams[pkg] = set.union(
                        ds, *(traverse(d) for d in ds if d in dependencies))
            return cumulative_upstreams[pkg]

        return pd.Series(dependencies.index, index=dependencies.index).map(
            traverse).rename(dependencies.name)

    return deps.apply(gen, axis=0)


def centrality(how, graph):
    # type: (str, nx.Graph) -> dict
    """ A wrapper for networkx centrality methods to allow for parametrization

    :param how: str, networkx centrality method
    :param graph: nx.Graph or nx.DiGraph
    :return: dict, {node_label: centrality_value}

    >>> centrality('degree', nx.Graph())
    {}
    >>> centrality('nonexistent', nx.Graph())
    Traceback (most recent call last)
    ...
    AssertionError: Unknown centrality measure: nonexistent
    >>> centrality('in_degree', nx.DiGraph())
    {}
    """
    if (not hasattr(nx, how) or not callable(getattr(nx, how))) \
            and hasattr(nx, how + "_centrality"):
        how += "_centrality"
    assert hasattr(nx, how) and callable(getattr(nx, how)), \
        "Unknown centrality measure: " + how
    return getattr(nx, how)(graph)


@fs_cache
def dependencies_centrality(ecosystem, centrality_type):
    """ Get centrality using dependencies graph
    :param ecosystem: {"npm"|"pypi"}
    :param centrality_type: networkx centrality method
    :return a dataframe, df.loc[package, month] = <float>

    >>> dc = dependencies_centrality("pypi", "in_degree")
    >>> isinstance(dc, pd.DataFrame)
    True
    >>> 50000 < len(dc) < 200000
    True
    >>> 150 < len(dc.columns) < 200
    True
    >>> dc.loc["django", "2017-12"] > 0  # 0.0554358
    True
    """
    log = logging.getLogger("ghd.common.dependencies_centrality")

    log.info("Collecting dependencies data..")
    uss = upstreams(ecosystem)

    def gen(stub):
        # stub = uss column
        log.info(stub.name)
        g = nx.DiGraph()
        for pkg, us in stub.items():
            if not us or pd.isnull(us):
                continue
            for u in us:  # u is upstream name
                g.add_edge(pkg, u)

        return pd.Series(centrality(centrality_type, g), index=stub.index)

    return uss.apply(gen, axis=0).fillna(0)


@d.memoize
def contributors(ecosystem, months=1):
    # type: (str) -> pd.DataFrame
    """ Get a historical list of developers contributing to ecosystem projects
    ~7s when cached (PyPI), few minutes otherwise

    :param ecosystem: {"pypi"|"npm"}
    :param months int(=1), use contributors for this number of last months
    :return: pd.DataFrame, index is projects, columns are months, cells are
        sets of str github usernames
    >>> c = contributors("pypi")
    >>> isinstance(c, pd.DataFrame)
    True
    >>> 50000 < len(c) < 200000
    True
    >>> 150 < len(c.columns) < 200
    True
    >>> len(c.loc["django", "2017-12"]) > 30  # 32, as of Jan 2018
    True
    """
    assert months > 0

    @fs_cache
    def _contributors(*_):
        start = START_DATES[ecosystem]
        columns = [dt.strftime("%Y-%m")
                   for dt in pd.date_range(start, 'now', freq="M")]

        def gen():
            log = logging.getLogger("ghd.common._contributors")

            for package, repo in package_urls(ecosystem).items():
                log.info(package)
                try:
                    s = scraper.commit_user_stats(repo).reset_index()[
                        ['authored_date', 'author']].groupby('authored_date').agg(
                        lambda df: set(df['author']))['author'].rename(
                        package).reindex(columns)
                except scraper.RepoDoesNotExist:
                    continue
                if months > 1:
                    s = pd.Series(
                        (set().union(*[c for c in s[max(0, i-months+1):i+1]
                                     if c and pd.notnull(c)])
                         for i in range(len(columns))),
                        index=columns, name=package)
                yield s

        return pd.DataFrame(gen(), columns=columns).applymap(
            lambda s: ",".join(str(u) for u in s) if s and pd.notnull(s) else "")

    return _contributors(ecosystem, months).applymap(
        lambda s: set(s.split(",")) if s and pd.notnull(s) else set())


def contributors_centrality(ecosystem, centrality_type):
    """ Get centrality measures for contributors graph.
    Doesn't make much sense for centrality_types other than degree
    12s

    >>> cc = contributors_centrality("pypi", "degree")
    >>> isinstance(cc, pd.DataFrame)
    True
    >>> 50000 < len(cc) < 200000
    True
    >>> 150 < len(cc.columns) < 200
    True
    >>> cc.loc["django", "2017-12"] > 40  # ~90 for the late 2017
    True
    """
    log = logging.getLogger("ghd.common.contributors_centrality")

    log.info("Getting contributors..")
    contras = contributors(ecosystem)
    # {in|out}_degree is not defined for undirected graphs

    log.info("Processing contributors centrality by month..")

    def gen(stub):
        # stub is a Series corresponding to a month
        log.info(stub.name)
        projects = defaultdict(set)  # projects[contributor] = set(projects)

        # first, find what projects every contributor contributed to
        for project, contributors_set in stub.items():
            if not contributors_set or pd.isnull(contributors_set):
                continue
            for contributor in contributors_set:
                projects[contributor].add(project)

        projects["-"] = set()
        g = nx.Graph()

        # then, for all pairs add an edge to the graph
        for project, contributors_set in stub.items():
            for contributor in contributors_set:
                for p in projects[contributor]:
                    if p > project:  # avoid duplicating edges
                        g.add_edge(project, p)

        ct = centrality(centrality_type, g)
        # ct is now nx.DegreeView, need to transform into dict
        return pd.Series(dict(ct), index=stub.index)

    return contras.apply(gen, axis=0).fillna(0)


def dead_projects(ecosystem, window, threshold):
    # definition of dead: <= 1 commit per month on average in a year
    # or, if commits data unavailable, over 1 year since last release
    es = get_ecosystem(ecosystem)
    deps = es.dependencies()
    commits = monthly_data(ecosystem, "commits")
    last_release = deps[['date']].groupby("name").max()
    death_date = pd.to_datetime(
        last_release['date'], format="%Y-%m-%d") + datetime.timedelta(days=365)
    death_str = death_date.dt.strftime("%Y-%m-%d")

    dead = pd.DataFrame([(death_str <= month).rename(month)
                         for month in commits.columns]).T
    sure_dead = (commits.T[::-1].rolling(
                 window=window, min_periods=1).max() < threshold)[::-1].T.astype(bool)
    dead.update(sure_dead)
    return dead


@fs_cache
def monthly_data(ecosystem, feature):
    # type: (str, str) -> pd.DataFrame
    """

    :param ecosystem: str, {"npm"|"pypi"}
    :param feature:
    :return: pd.DataFrame, df.loc[project, month] = <feature_value>
    """
    urls = package_urls(ecosystem)
    idx = [dt.strftime("%Y-%m")
           for dt in pd.date_range(START_DATES[ecosystem], 'now', freq="M")]

    full_handlers = {
        'upstreams': lambda es: count_values(upstreams(es)),
        't_upstreams': lambda es: count_values(cumulative_dependencies(upstreams(es))),
        'downstreams': lambda es: count_values(downstreams(es)),
        't_downstreams': lambda es: count_values(cumulative_dependencies(downstreams(es))),
        'backporting': backporting,
        'dc_katz': lambda es: dependencies_centrality(es, 'katz'),
        'dc_closeness': lambda es: dependencies_centrality(es, "closeness"),
        'cc_degree': lambda es: contributors_centrality(es, "degree"),
    }

    project_handlers = {
        # COMMIT METRICS
        'commits': lambda url: scraper.commit_stats(url),
        'contributors': lambda url: scraper.commit_users(url),
        'q50': lambda url: scraper.contributions_quantile(url, 0.5),
        'q70': lambda url: scraper.contributions_quantile(url, 0.7),
        'q90': lambda url: scraper.contributions_quantile(url, 0.9),
        'gini': lambda url: scraper.commit_gini(url),
        # ISSUES METRICS
        'issues': scraper.new_issues,
        'non_dev_issues': scraper.non_dev_issue_stats,
        'submitters': scraper.submitters,
        'non_dev_submitters': scraper.non_dev_submitters,
        # EMAILS
        'commercial': scraper.commercial_involvement,
        'university': scraper.university_involvement,
    }

    if feature in full_handlers:
        return full_handlers[feature](ecosystem).T.reindex(
            idx, fill_value=0).T.reindex(urls.index, fill_value=0)
    elif feature in project_handlers:
        def gen():
            log = logging.getLogger(feature)
            for project_name, url in urls.items():
                log.info(project_name)
                try:
                    yield project_handlers[feature](url).rename(project_name)
                except scraper.RepoDoesNotExist:
                    continue

        return pd.DataFrame(gen(), columns=idx).fillna(0)
    raise ValueError("Unknown feature: " + feature)


@fs_cache
def survival_data(ecosystem, start_date="2005", end_date="2017-12", smoothing=1):
    """ The main method of this module.
    These data is to be used by Cox regression

    :param ecosystem: ("npm"|"pypi")
    :param start_date: str, remove projects started before this date
    :param end_date: str, rermove observations after this date
    :param smoothing:  number of month to average over
    :return: pd.Dataframe with columns:
         age, date, project, dead, last_observation
         commercial, university, org, license,
         commits, contributors, q50, q70, q90, gini,
         issues, non_dev_issues, submitters, non_dev_submitters
         downstreams, upstreams, transitive downstreams, transitive upstreams,
         contributors centrality,
         dependencies centrality

    This dataset takes hours to days to compute, so not tested
    """
    log = logging.getLogger("ghd.survival")
    death_window = 12
    death_threshold = 1.0

    # ensure there is enough to chip off for smoothing at the end
    assert smoothing <= death_window, "Smoothing window is too big"

    cs = monthly_data(ecosystem, "commits")

    # ensure there is enough to chip off for smoothing in the beginning
    assert (cs.columns < start_date).sum() > smoothing, "Use later start_date"

    # drop everything after end_date (date when dataset was collected)
    cs = cs.loc[:, :end_date]

    # drop deleted projects (after fillna they have 0 commits in total)
    cs = cs[cs.sum(axis=1) > 0]
    cs.index.name = "name"
    cs.columns.name = "month"

    # we need package info to get license
    es = get_ecosystem(ecosystem)
    pkginfo = es.packages_info()
    licenses = pkginfo["license"].map(parse_license)

    proj_info = user_info(ecosystem)

    # drop projects for which we don't have user/org info (i.e. deleted)
    cs = cs.reindex(proj_info.index)

    # at this point cs is a pd.Dataframe, cs.loc[project, month] = value
    # now let's convert it into a single column with index (project, month)
    df = pd.DataFrame(cs.T.unstack().rename('commits'))

    # make sure all numeric features included and supported by monthly_data
    # features outside of the list will not be smoothed
    features = (
        'contributors', 'q90',
        'issues', 'non_dev_issues', 'submitters', 'non_dev_submitters',
        'upstreams', 't_upstreams', 'downstreams', 't_downstreams',
        'dc_katz', 'dc_closeness',
        'backporting',
        'cc_degree',
        'university', 'commercial'
    )
    # subset of features not to be smoothed (e.g. boolean values)
    no_smoothing = {'backporting'}
    for feature in features:
        log.info(feature)
        df[feature] = monthly_data(
            ecosystem, feature).T.unstack().rename(feature)

    # at this point we don't need multiindex anymore
    df = df.reset_index()

    # first commit dates, will be used later to cut leading zero observations
    fcd = df.loc[df["commits"] > 0, ["name", 'month']
                 ].groupby('name').first()["month"]
    # drop observations before first commit date
    # ... and projects started before start_date
    # e.g. Django and numpy were started before PyPI so data are incomplete
    df = df[(df['name'].map(fcd) <= df["month"]) &
            (df['name'].map(fcd) > start_date)]

    def age_gen():
        last_month = "9999"
        age = 0
        for month in df["month"]:
            age = (age + 1) * (last_month < month)
            yield age
            last_month = month

    # important to do it before dropping after-death observations
    # otherwise can catchu up on a project died before the next has started
    df["age"] = list(age_gen())

    # find first death and drop everything afterwards
    # it's ok to jam last <window> month of observations, we'll discard it later
    df["dead"] = (
        df["commits"][::-1].rolling(window=death_window, min_periods=1).mean()
        < death_threshold)[::-1].shift(-1).fillna(method='ffill')

    if smoothing > 1:
        for feature in features:
            if feature not in no_smoothing:
                df[feature] = df[feature].rolling(
                    window=smoothing, min_periods=1).mean()

    # drop last <death_window> month
    # (incomplete observation + contaminated by the next project data)
    df = df[df["month"] < cs.columns[-death_window]]

    # drop everything after first death (even later revivals, if any)
    death = df.loc[
        df["dead"], ["name", "month"]].groupby("name").first()["month"]
    df = df[df["name"].map(death).fillna("9999") >= df["month"]]

    # doing it only so late to save couple milliseconds on deleted rows
    df["org"] = df["name"].map(proj_info["org"])
    df["license"] = df["name"].map(licenses)

    df["last_observation"] = df["name"] != df["name"].shift(-1)

    # convert bool columns to int (for R processing)
    for feature in ('org', 'dead', 'last_observation'):
        df[feature] = df[feature].astype(int)

    # resample to have only n-th observation (n=smoothing) and the last one
    df = df[((df["age"] % smoothing) == 0) | df["last_observation"]]

    return df
